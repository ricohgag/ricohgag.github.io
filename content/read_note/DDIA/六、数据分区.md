+++
title = 'DDIA-Part6: 数据分区'
date = 2020-12-06T11:14:33+08:00
draft = true
+++

每条数据值属于某个特定分区，不同分区可以放在一个无共享集群的不同节点上。分
区的主要目的是将数据和查询负载均匀分布在所有节点上，提高可扩展性。

如果分区不均匀，会出现**倾斜**。倾斜会导致分区效率严重下降，在极端情况下，所有的负载可能会集中在一个分区节点上，这种负载严重不成比例的分区即成为**系统热点**。

### 数据分区与数据复制

分区和复制结合使用，每个分区在多个节点上都存有副本。某条数据属于特定的分区，但同样的数据会以复制的方式保存在不同的节点上以提高系统的容错性。

## 键-值数据的分区

#### 基于关键字区间分区

为每个分区分配一段连续的关键字或关键字区间范围，分区内可以按关键字排序保存，可以支持区间查询。

缺点是某些访问模式会导致热点（时间戳），可以使用多个字段组合作为关键字，将热点数据隔离。

#### 基于关键字哈希分区

一个好的哈希函数可以处理数据倾斜并使其均匀分布，为每个分区分配一个hash范围，关键字根据哈希值划分到不同分区中。

通过哈希分区，我们失去了区间查询特性。可以通过复合主键建立高效的区间查询（第一部分用于哈希，其他列用作组合索引对数据排序）。

#### 负载倾斜与热点

基于哈希的分区方法可以减轻热点，但无法做到完全避免。一个极端情况是所有的读/写都是针对同一个关键字，最终请求都将被路由到同一个分区。（【读】如社交网站上一些名人发布一些热点事件可能会引发一场访问风暴，【写】大量用户对这件热点事件id进行评论）。

这种情况只能靠应用层来减轻倾斜程度，比如在关键字后增加随机数，从而分配到不同分区。但是滞后的读取都必须从所有分区中读取数据再合并。这通常只对少量的热点关键字才有意义，对写入吞吐量低的大多数关键字都是不必要的开销。

## 分区与二级索引

二级索引通常不能唯一标识一条记录，通常用来加速特定值的查询，如查询红色的汽车。

二级索引是关系型数据库的必备特性，在文档数据库中应用也非常普遍。但是因为其复杂性，许多键-值存储并不支持二级索引（如HBase和Voldemort），也有些也开始支持二级索引。二级索引技术还是Solr和ElasticSearch等全文索引服务器的根本。

二级索引的主要挑战是它们不能规整的映射到分区中，主要有两种方法支持对二级索引分区。

#### 基于文档分区的二级索引

每个分区完全独立，各自维护自己的二级索引，且只负责自己分区内的文档。每当需要写数据库时（新增、修改、删除），只需要处理包含文档id的分区。这还总给你也被成为本地索引，而不是全局索引。

读取时：如果没有做特殊处理，二级索引的字段某个值大概率不会都在一个分区中。因此查询的时候需要查询所有分区然后合并所有返回的结果，这种方式也被成为分散/聚集。这种二级索引查询代价高昂，即使采用了并行查询，也容易导致读延迟显著放大。MongoDB、Riak、Cassandra、Elasticsearch、SolrCloud和VoltDB都支持基于文档分区的二级索引。

数据库供应商建议用户自己构建合适的分区方案，尽量由单个分区满足二级索引查询，但是查询可能会引用多个二级索引。

#### 基于词条的二级索引分区

我们可以对所有的数据构建全局索引，而不是每个分区维护自己的本地索引。将全局索引分区使得分区均衡，且可以与数据关键字采用不用的分区策略。直接通过关键词划分索引可以支持高效的区间查询，而采用哈希可以更均匀的划分分区。

全局索引的优点是，它的读取更高效，不需要对所有分区都执行一遍查询，客户端只需要向包含词条的分区发出读请求。

全局索引的缺点是，写入速度较慢且非常复杂，单个文档更新时可能设计多个二级索引，而二级索引又可能在不同的分区/节点上，这样会导致写入变慢。

理想情况下，索引应该保持最新，即写入数据要立即反映在最新的索引上。但是对于词条分区来讲，这需要一个跨多分区的分布式事务的支持。写入速度会受到极大影响，所以现有数据库都是异步更新全局二级索引。

## 分区再平衡

随着时间退役，数据库会出现某些变化：

- 查询压力增大，需要更多的CPU来处理负载。

- 数据规模增加，需要更多的磁盘和内存来存储数据。

- 节点可能出现故障，需要其他机器来接管失效节点。

这些变化都会要求数据和请求从一个节点转移到另一个节点。这个迁移负载的过程被成为再平衡（动态平衡）。

分区再平衡需要满足：

- 平衡之后，负载、数据存储、读写请求应该在集群范围更均匀的分布。

- 再平衡执行过程中，数据库应该可以继续正常提供读写服务。

- 避免不要的负载迁移，以加快动态平衡，并尽量减少网络和磁盘IO影响。

#### 动态再平衡策略

- 取模：如果节点数发生变化，会导致数据大量频繁迁移。

- 固定数量的分区

如果数据集的总规模高度不确定或可变，分区的数量就难以确定。如果分区数量小，分区数据量大，每次再平衡和节点故障恢复的代价就很大。如果分区数太大，分区数据量小，就会产生过多的开销。

- 动态分区

当分区内数据量超过一个可配阈值，它就拆分为两个分区，每个包含之前的一半数据量。如果大量数据被删除，它将其与相邻分区合并，类似于**B树的分裂操作**。对于HBase，分区文件的传输需要借助HDFS（底层分布式文件系统）。

动态分区的优点是分区数量可以自动适配数据总量，适用于关键字区间分区，也适用于基于哈希的分区策略。

- 按节点比例分区

每个节点固定数量的分区，当节点数不变，每个分区大小与数据集大小成正比的增长关系。当节点加入集群，随机的对固定数量的现有分区进行分裂，然后拿走这些分区一半的数据量。随机选择分区边界的前提要求采用基于哈希分区。这种方法符合一致性哈希算法，一些新的哈希函数也能达到类似的效果。

#### 自动再平衡操作

动态平衡自动还是手动执行。全自动的再平衡更加方便，但是可能会出现难以预测的情况，如果执行中出现异常会使得网络或节点的负载过重，并影响其他请求的性能。

自动平衡与自动故障检测结合也有一些风险，当节点负载过重，对请求的响应暂时受到影响，会被其他节点认为失效。接下来激活自动平衡转移其负载，会更加加重该节点、其他节点和网络的负载，甚至可能导致级联式的失效扩散。

管理员介入到再平衡是个更好的选择，虽然会比全自动过程响应更慢，但是可以有效的防止意外。

## 请求路由

数据已经分布到多个节点上，当客户端发送请求时，知道应该连接到哪个节点。如果发生了分区再平衡，分区与节点之间的关系还会随之变化（服务发现问题）。

1. 允许客户端链接任意节点。如果某节点拥有所请求分区，直接处理该请求。否则请求转发到下一个合适的节点。

2. 将所有客户端请求发送到一个路由层，后者负责将请求转发到对应的分区节点上。路由层本身不处理任何请求，仅充当分区感知的负载均衡器。

3. 客户端感知分区和节点的分配关系。此时，客户端可以直接链接目标节点，不需要任何中介。

核心问题是：作出路由决策的组件如何知道分区与节点的对应关系及关系的变化情况。

许多分布式数据系统依靠独立的协调服务（ZooKeeper）跟踪集群范围内的元数据，每个节点都向ZooKeeper注册自己，Zookeeper维护了分区和节点的最终映射关系。其他参与者（路由层或分区感知的客户端）可以向ZooKeeper订阅此信息。分区一旦发生改变，添加或删除节点，ZooKeeper就会主动通知路由层或客户端，使得路由信息保持最新状态。

例如，LinkedIn的Espresso使用Helix（底层为Zookeeper的路由层）进行集群管理，实现了请求路由层。HBase，SolrCloud和Kafka使用ZooKeeper来跟踪分区分配情况（路由感知的客户端）。MongoDB依赖自己的配置服务器和mongos守护进程充当路由层。Cassandra和Riak采用了不同的方法，它们在节点之间使用gossip协议来同步集群状态的变化。请求可以发送到任意节点，该节点负责将其转发到目标分区节点。这种方式增加了数据库节点的复杂性，但是避免了对ZooKeeper之类的外部组件的依赖（RabbitMQ）。

#### 并行执行查询

对于大规模并行处理（MPP）这类主要用于数据分析的关系型数据库，在查询类型放买呢要复杂得多。典型的数据仓库查询包含多个联合、过滤、分组和聚合操作。MPP查询优化器会将复杂的查询分解成许多执行阶段和分区，以便在集群的不同节点上并行执行。尤其是涉及全表扫描这样的查询操作，可以通过并行执行获益颇多。

